{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用tensorflow2.0卷积神经网络进行卫星图片分类实例操作详解\n",
    "来源：https://blog.csdn.net/lys_828/article/details/101322246"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.0.0, GPU is False\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print('Tensorflow version: {}, GPU is {}'.format(tf.__version__, tf.test.is_gpu_available()))\n",
    "if tf.test.is_gpu_available():\n",
    "    gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "    #下面的方式是设置Tensorflow固定消耗GPU:0的2GB显存（该设置对代码缺陷（GPU需求内存超物理GPU内存时异常退出）不起作用）。\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import IPython.display as display\n",
    "import datetime\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "class MyCnn:\n",
    "    def __init__(self, width = 256 , height = 256):\n",
    "        #设定图片预处理的尺寸(width,height)\n",
    "        super(MyCnn, self).__init__()\n",
    "        self.time_start = datetime.datetime.now()\n",
    "        self.time_keep = 0\n",
    "        self.IMAGE_WIDTH = width\n",
    "        self.IMAGE_HEIGHT = height\n",
    "        self.model = tf.keras.Sequential()   #顺序模型\n",
    "        if width == 256 and height == 256:\n",
    "            #正常模式（有充足的GPU内存或CPU模式）\n",
    "            self.model.add(tf.keras.layers.Conv2D(64, (3, 3), input_shape=(width, height, 3), activation='relu'))\n",
    "            self.model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "            self.model.add(tf.keras.layers.MaxPooling2D())\n",
    "            self.model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "            self.model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "            self.model.add(tf.keras.layers.MaxPooling2D())\n",
    "            self.model.add(tf.keras.layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "            self.model.add(tf.keras.layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "            self.model.add(tf.keras.layers.MaxPooling2D())\n",
    "            self.model.add(tf.keras.layers.Conv2D(512, (3, 3), activation='relu'))\n",
    "            self.model.add(tf.keras.layers.MaxPooling2D())\n",
    "            self.model.add(tf.keras.layers.Conv2D(512, (3, 3), activation='relu'))\n",
    "            self.model.add(tf.keras.layers.MaxPooling2D())\n",
    "            self.model.add(tf.keras.layers.Conv2D(1024, (3, 3), activation='relu'))\n",
    "            self.model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "            self.model.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
    "            self.model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "            self.model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "        else:\n",
    "            #适应小内存，input_shape=(128,128,3)\n",
    "            self.model.add(tf.keras.layers.Conv2D(64, (3, 3), input_shape=(width, height, 3), activation='relu'))\n",
    "            #self.model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "            self.model.add(tf.keras.layers.MaxPooling2D())\n",
    "            self.model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "            #self.model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "            self.model.add(tf.keras.layers.MaxPooling2D())\n",
    "            self.model.add(tf.keras.layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "            #self.model.add(tf.keras.layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "            self.model.add(tf.keras.layers.MaxPooling2D())\n",
    "            self.model.add(tf.keras.layers.Conv2D(512, (3, 3), activation='relu'))\n",
    "            self.model.add(tf.keras.layers.MaxPooling2D())\n",
    "            #self.model.add(tf.keras.layers.Conv2D(512, (3, 3), activation='relu'))\n",
    "            #self.model.add(tf.keras.layers.MaxPooling2D())\n",
    "            self.model.add(tf.keras.layers.Conv2D(1024, (3, 3), activation='relu'))\n",
    "            self.model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "            self.model.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
    "            self.model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "            self.model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    def loaddata(self, data_dir, batch_size = 32, num_parallel_calls=tf.data.experimental.AUTOTUNE, test_rate = 0.2):\n",
    "        self.all_image_paths, self.all_image_labels, self.image_count, self.label_names = self.loadfiles(data_dir)\n",
    "        self.train_data , self.train_count , self.test_data , self.test_count = self.datasplit(self.all_image_paths , batch_size = batch_size , num_parallel_calls = num_parallel_calls , test_rate = test_rate)\n",
    "        self.steps_per_epoch = self.train_count//batch_size\n",
    "        self.validation_steps = self.test_count//batch_size\n",
    "    def compile(self):\n",
    "        self.model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['acc']\n",
    "                     )\n",
    "    def fit(self , epochs = 30):\n",
    "        self.history = self.model.fit(self.train_data, epochs=epochs, steps_per_epoch=self.steps_per_epoch, validation_data=self.test_data, validation_steps=self.validation_steps)\n",
    "        self.time_stop = datetime.datetime.now()\n",
    "        self.time_keep = self.time_stop - self.time_start\n",
    "    def loadmodel(self , path):\n",
    "        #模型的加载\n",
    "        try:\n",
    "            model_exist = tf.keras.models.load_model(path)\n",
    "        except:\n",
    "            return False;\n",
    "        self.model = model_exist\n",
    "        return True;\n",
    "    def savemodel(self , path):\n",
    "        #模型的保存\n",
    "        self.model.save(path)\n",
    "        #保存tensorflow格式的参数值\n",
    "        #model.save_weights(SAVE_PATH + 'model')\n",
    "        #保存keras格式的参数值（权重）\n",
    "        #model.save_weights(SAVE_PATH + 'model',save_format='HDF5')\n",
    "    def from_json(self, json_str):\n",
    "        self.model = tf.keras.models.model_from_json(json_str) \n",
    "    def to_json(self):\n",
    "        # 序列化成json\n",
    "        json_str = self.model.to_json()\n",
    "        pprint.pprint(json.loads(json_str))\n",
    "    def showhistory(self):\n",
    "        try:\n",
    "            r=object.__getattribute__(self, 'history')\n",
    "        except:\n",
    "            r=None\n",
    "\n",
    "        if r == None:\n",
    "            print('no history')\n",
    "        else:\n",
    "            print(self.history.history.keys())\n",
    "            #显示正确率\n",
    "            plt.plot(self.history.epoch, self.history.history.get('acc'), label='acc')\n",
    "            plt.plot(self.history.epoch, self.history.history.get('val_acc'), label='val_acc')\n",
    "            plt.plot(self.history.epoch, np.array(self.history.history.get('acc')) * np.array(self.history.history.get('val_acc')), label='cc')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            #显示错误率\n",
    "            plt.plot(self.history.epoch, self.history.history.get('loss'), label='loss')\n",
    "            plt.plot(self.history.epoch, self.history.history.get('val_loss'), label='val_loss')\n",
    "            plt.plot(self.history.epoch, np.array(self.history.history.get('loss')) * np.array(self.history.history.get('val_loss')), label='ss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    def imagepred(self, image_path , label , withshow = True):\n",
    "        #模型预测\n",
    "        image = self.load_and_preprocess_image(image_path)\n",
    "        if withshow:\n",
    "            plt.imshow(image)\n",
    "            plt.grid(False)\n",
    "            plt.xlabel(self.caption_image(label))\n",
    "            plt.show()\n",
    "\n",
    "        image1 = tf.io.read_file(image_path)\n",
    "        image1 = tf.image.decode_jpeg(image1, channels=3)\n",
    "        image1 = tf.image.resize(image1, [self.IMAGE_WIDTH, self.IMAGE_HEIGHT])\n",
    "        image1 = tf.cast(image1, tf.float32)\n",
    "        image1 = image1/255.0  # normalize to [0,1] range\n",
    "\n",
    "        image2 = np.array([image1.numpy()])\n",
    "\n",
    "        result = self.model.predict(image2)\n",
    "\n",
    "        result_value = tf.argmax(result,1).numpy()\n",
    "        if label == -1:\n",
    "            print('pred label={},{}({})'.format(result_value[0] , self.caption_image(result_value[0]) , image_path))\n",
    "        elif result_value != label:\n",
    "            print('pred label=' , result_value[0] , self.caption_image(result_value[0]), '(error)') \n",
    "        else:\n",
    "            print('pred label=' , result_value[0] , self.caption_image(result_value[0])) \n",
    "    def imagepredi(self, index, withshow = True):\n",
    "        #获取要预测的图片路径及标签，并调用imagepred进行预测校验\n",
    "        image_path = self.all_image_paths[index]\n",
    "        label = self.all_image_labels[index]\n",
    "        return self.imagepred(image_path , label , withshow = withshow)\n",
    "    def loadfiles(self,data_dir , randomflag = True):\n",
    "        #根据相对目录路径，读取文件清单，标签清单，文件数（文件被random打乱）\n",
    "        #一级目录为airplan、lake，分别表示机场、湖泊，二级目录为对应的图片文件\n",
    "        data_root = pathlib.Path(data_dir)\n",
    "\n",
    "        #print(data_root)\n",
    "        #for item in data_root.iterdir():\n",
    "        #    print(item)\n",
    "\n",
    "        all_image_paths = list(data_root.glob('*/*'))\n",
    "        image_count = len(all_image_paths)\n",
    "\n",
    "        #print(all_image_paths[:3],all_image_paths[-3:])\n",
    "\n",
    "        #打乱顺序\n",
    "        all_image_paths = [str(path) for path in all_image_paths]\n",
    "        if randomflag:\n",
    "            random.shuffle(all_image_paths)\n",
    "            #print(all_image_paths[:5])\n",
    "\n",
    "        label_names = sorted(item.name for item in data_root.glob('*/') if item.is_dir())\n",
    "        #print(label_names)\n",
    "        #对排序后的label按目录名进行排序\n",
    "        label_to_index = dict((name, index) for index,name in enumerate(label_names))\n",
    "        #print(label_to_index)\n",
    "\n",
    "        #根据文件的目录名生成label结果集\n",
    "        all_image_labels = [label_to_index[pathlib.Path(path).parent.name] for path in all_image_paths]\n",
    "        #print(all_image_labels[:5])\n",
    "\n",
    "        return all_image_paths, all_image_labels, image_count, label_names\n",
    "    def checkfile(self):\n",
    "        #随机抽查几张图片及其标签值\n",
    "        for n in range(3):\n",
    "            image_index = random.choice(range(len(self.all_image_paths)))\n",
    "            display.display(display.Image(self.all_image_paths[image_index]))\n",
    "            print(image_index, self.caption_image(self.all_image_labels[image_index]))\n",
    "            #print()\n",
    "        #加载和格式化图像\n",
    "\n",
    "        img_path = self.all_image_paths[0]\n",
    "        print(img_path)\n",
    "\n",
    "        img_raw = tf.io.read_file(img_path)\n",
    "        print(repr(img_raw)[:100]+\"...\")\n",
    "\n",
    "        img_tensor = tf.image.decode_image(img_raw)\n",
    "        print(img_tensor.shape)\n",
    "        print(img_tensor.dtype)\n",
    "\n",
    "        img_tensor = tf.cast(img_tensor, tf.float32)\n",
    "        img_final = img_tensor/255.0\n",
    "        print(img_final.shape)\n",
    "        print(img_final.numpy().min())\n",
    "        print(img_final.numpy().max())\n",
    "    def loadcheckfiles(self,data_dir):\n",
    "        data_root = pathlib.Path(data_dir)\n",
    "\n",
    "        all_image_paths = list(data_root.glob('*'))\n",
    "        all_image_paths = [str(path) for path in all_image_paths]\n",
    "        return all_image_paths\n",
    "    def caption_image(self,label):\n",
    "        #根据标签值获取标签对应的名称\n",
    "        return {0: 'airplane', 1: 'lake'}.get(label)\n",
    "    def load_and_preprocess_image(self,path):\n",
    "        #加载并预处理图像，使之值介于[0,1]范围内\n",
    "        image = tf.io.read_file(path)\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.image.resize(image, [self.IMAGE_WIDTH, self.IMAGE_HEIGHT])\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image = image/255.0  # normalize to [0,1] range\n",
    "        return image\n",
    "    def imageshow(self,imageindex):\n",
    "        #按文件索引进行显示图片\n",
    "        image_path = self.all_image_paths[imageindex]\n",
    "        label = self.all_image_labels[imageindex]\n",
    "\n",
    "        plt.imshow(self.load_and_preprocess_image(image_path))\n",
    "        plt.grid(False)\n",
    "        plt.xlabel(self.caption_image(label))\n",
    "        print()\n",
    "    def datasplit(self, data , batch_size = 32, num_parallel_calls=tf.data.experimental.AUTOTUNE, test_rate = 0.2):\n",
    "        path_ds = tf.data.Dataset.from_tensor_slices(data)\n",
    "        image_ds = path_ds.map(self.load_and_preprocess_image, num_parallel_calls=num_parallel_calls)\n",
    "        label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(self.all_image_labels, tf.int64))\n",
    "        image_label_ds = tf.data.Dataset.zip((image_ds, label_ds))\n",
    "        #for label in label_ds.take(10):\n",
    "        #    print(label_names[label.numpy()])\n",
    "        #print(image_label_ds)\n",
    "        image_count = len(self.all_image_paths)\n",
    "        test_count = int(image_count*test_rate)\n",
    "        train_count = image_count - test_count\n",
    "        train_data = image_label_ds.skip(test_count)\n",
    "        test_data = image_label_ds.take(test_count)\n",
    "\n",
    "        train_data = train_data.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=train_count))\n",
    "        #可以改成以下写法（待定）\n",
    "        #train_data = train_data.shuffle(buffer_size=train_count)\n",
    "        #train_data = train_data.repeat(10)#epochs\n",
    "        #train_data = train_data.apply(train_data)\n",
    "        \n",
    "        train_data = train_data.batch(batch_size)\n",
    "        train_data = train_data.prefetch(buffer_size=num_parallel_calls)\n",
    "        #print(train_data)\n",
    "\n",
    "        test_data = test_data.batch(batch_size)\n",
    "\n",
    "        return train_data , train_count , test_data , test_count\n",
    "    def summary(self):\n",
    "        self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './2_class'\n",
    "MODEL_PATH = './airplan-lake.model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 128\n",
    "IMAGE_HEIGHT = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-603e3fc4b90e>:241: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n"
     ]
    }
   ],
   "source": [
    "cnn = MyCnn(IMAGE_WIDTH , IMAGE_HEIGHT)\n",
    "cnn.loaddata(data_dir , batch_size = BATCH_SIZE , num_parallel_calls = AUTOTUNE , test_rate = 0.2)\n",
    "cnn.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.loadmodel(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 35 steps, validate for 8 steps\n",
      "Epoch 1/10\n",
      "35/35 [==============================] - 63s 2s/step - loss: 0.1399 - acc: 0.9616 - val_loss: 0.0496 - val_acc: 0.9844\n",
      "Epoch 2/10\n",
      "35/35 [==============================] - 60s 2s/step - loss: 0.0837 - acc: 0.9705 - val_loss: 0.0471 - val_acc: 0.9844\n",
      "Epoch 3/10\n",
      "35/35 [==============================] - 62s 2s/step - loss: 0.0695 - acc: 0.9804 - val_loss: 0.0651 - val_acc: 0.9883\n",
      "Epoch 4/10\n",
      "35/35 [==============================] - 61s 2s/step - loss: 0.0551 - acc: 0.9839 - val_loss: 0.0432 - val_acc: 0.9883\n",
      "Epoch 5/10\n",
      "35/35 [==============================] - 61s 2s/step - loss: 0.0787 - acc: 0.9786 - val_loss: 0.0871 - val_acc: 0.9688\n",
      "Epoch 6/10\n",
      "35/35 [==============================] - 60s 2s/step - loss: 0.0783 - acc: 0.9821 - val_loss: 0.0664 - val_acc: 0.9766\n",
      "Epoch 7/10\n",
      "35/35 [==============================] - 62s 2s/step - loss: 0.0632 - acc: 0.9795 - val_loss: 0.0344 - val_acc: 0.9922\n",
      "Epoch 8/10\n",
      "35/35 [==============================] - 62s 2s/step - loss: 0.0965 - acc: 0.9732 - val_loss: 0.1480 - val_acc: 0.9727\n",
      "Epoch 9/10\n",
      "35/35 [==============================] - 61s 2s/step - loss: 0.1021 - acc: 0.9768 - val_loss: 0.0483 - val_acc: 0.9883\n",
      "Epoch 10/10\n",
      "35/35 [==============================] - 59s 2s/step - loss: 0.1717 - acc: 0.9580 - val_loss: 0.0626 - val_acc: 0.9844\n"
     ]
    }
   ],
   "source": [
    "cnn.fit(epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./airplan-lake.model/assets\n"
     ]
    }
   ],
   "source": [
    "cnn.savemodel(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 6s 741ms/step - loss: 0.0414 - acc: 0.9922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.041400633519515395, 0.9921875]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#模型评估（直接采用验证集来评估，结果应该跟val_acc一致）\n",
    "steps = cnn.validation_steps\n",
    "cnn.model.evaluate(cnn.test_data, steps= steps) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no history\n"
     ]
    }
   ],
   "source": [
    "cnn.showhistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred label= 0 airplane\n",
      "pred label= 0 airplane\n",
      "pred label= 0 airplane\n",
      "pred label= 1 lake\n",
      "pred label= 1 lake\n",
      "pred label= 1 lake\n",
      "pred label= 1 lake\n",
      "pred label= 1 lake\n",
      "pred label= 1 lake\n",
      "pred label= 0 airplane\n",
      "pred label= 1 lake\n",
      "pred label= 0 airplane\n",
      "pred label= 1 lake\n",
      "pred label= 1 lake\n",
      "pred label= 0 airplane\n",
      "pred label= 0 airplane\n",
      "pred label= 0 airplane\n",
      "pred label= 1 lake\n",
      "pred label= 0 airplane\n",
      "pred label= 1 lake\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    cnn.imagepredi(i, withshow = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check image count =  20\n",
      "pred label=0,airplane(4_check\\机场21.jpg)\n",
      "pred label=0,airplane(4_check\\机场22.jpg)\n",
      "pred label=1,lake(4_check\\机场23.jpg)\n",
      "pred label=0,airplane(4_check\\机场24.jpg)\n",
      "pred label=0,airplane(4_check\\机场25.jpg)\n",
      "pred label=0,airplane(4_check\\机场26.jpg)\n",
      "pred label=0,airplane(4_check\\机场27.jpg)\n",
      "pred label=0,airplane(4_check\\机场28.jpg)\n",
      "pred label=0,airplane(4_check\\机场29.jpg)\n",
      "pred label=1,lake(4_check\\机场30.jpg)\n",
      "pred label=1,lake(4_check\\湖泊21.jpg)\n",
      "pred label=0,airplane(4_check\\湖泊22.jpg)\n",
      "pred label=1,lake(4_check\\湖泊23.jpg)\n",
      "pred label=1,lake(4_check\\湖泊24.jpg)\n",
      "pred label=1,lake(4_check\\湖泊25.jpg)\n",
      "pred label=1,lake(4_check\\湖泊26.jpg)\n",
      "pred label=1,lake(4_check\\湖泊27.jpg)\n",
      "pred label=1,lake(4_check\\湖泊28.jpg)\n",
      "pred label=1,lake(4_check\\湖泊29.jpg)\n",
      "pred label=1,lake(4_check\\湖泊30.jpg)\n"
     ]
    }
   ],
   "source": [
    "CHECK_PATH = './4_check'\n",
    "check_image_paths = cnn.loadcheckfiles(CHECK_PATH)\n",
    "print('check image count = ', len(check_image_paths))\n",
    "for f in check_image_paths:\n",
    "    cnn.imagepred(f , -1, withshow = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
